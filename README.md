# RAGnarok ğŸ”¥

**RAGnarok** is a fully-featured local Retrieval-Augmented Generation (RAG) system that enables intelligent document-based Q&A. Upload your PDFs and ask questions - RAGnarok will search through your documents to provide contextual answers with source attribution.

## âœ¨ Key Features

### ğŸ§  Smart Document Processing
- **Automatic PDF Processing**: Upload PDFs and they're immediately processed through the RAG pipeline
- **Text Chunking**: Documents are intelligently split into semantic chunks with overlap
- **Vector Embeddings**: Uses sentence-transformers for high-quality semantic embeddings
- **Persistent Storage**: ChromaDB vector database with persistent storage

### ğŸ” Intelligent Question Answering  
- **Semantic Search**: Find relevant content using vector similarity search
- **Source Attribution**: See exactly which documents contributed to each answer
- **Context-Aware Responses**: LLM receives relevant document chunks as context
- **Graceful Fallback**: Provides general knowledge answers when no relevant content found

### ğŸ¨ Modern Web Interface
- **Beautiful UI**: Fire-themed modern interface with collapsible sidebar
- **Real-time Chat**: Interactive chat interface with message history
- **Processing Status**: Visual indicators showing which PDFs have been processed
- **Responsive Design**: Works on desktop and mobile devices

### ğŸ›  Developer Features
- **REST API**: Complete Flask-based API for all functionality
- **Admin Tools**: Flush and reprocess endpoints for document management
- **Docker Compose**: One-command deployment with all services
- **Database Migration**: Automatic schema updates

## ğŸš€ Tech Stack

- **Backend:** Python (Flask, SQLAlchemy)
- **LLM:** Ollama (Llama3) - runs locally
- **Embeddings:** Sentence Transformers (all-MiniLM-L6-v2)
- **Vector DB:** ChromaDB for semantic search
- **Database:** PostgreSQL for metadata
- **Frontend:** React.js with modern styling
- **PDF Processing:** pdfplumber for text extraction
- **Containerization:** Docker, Docker Compose

## ğŸš€ Getting Started

### Prerequisites
- Docker & Docker Compose
- At least 4GB RAM (for Ollama LLM)
- 10GB+ free disk space

### Quick Start
1. **Clone the repository**
   ```bash
   git clone <your-repo-url>
   cd RAGnarok
   ```

2. **Start all services**
   ```bash
   docker-compose up --build
   ```
   
   **First run notes:**
   - Downloads Docker images for Python, Node, PostgreSQL, and Ollama
   - Ollama will download the Llama3 model (~4GB) on first use
   - May take 10-15 minutes for initial setup

3. **Access the application**
   - **Frontend:** http://localhost:3000
   - **Backend API:** http://localhost:5000
   - **Database:** localhost:5432 (postgres/postgres)

### First Steps
1. ğŸ“„ Upload a PDF using the sidebar upload interface
2. â³ Wait for processing (you'll see "âœ“ Processed" with chunk count)
3. ğŸ’¬ Ask questions about your document in the chat interface
4. ğŸ¯ See source attribution and relevant context in responses

## ğŸ“– Usage Guide

### Uploading Documents
- Click the upload area in the left sidebar
- Select PDF files (multiple uploads supported)
- Documents are automatically processed through the RAG pipeline
- Processing status shows: "âœ“ Processed (X chunks)" or "âš  Not processed"

### Asking Questions
- Type questions in the chat interface
- RAGnarok searches your uploaded documents for relevant context
- Responses include:
  - **ğŸ“š Sources**: Which documents were referenced
  - **â„¹ï¸ Context indicators**: Whether answer came from documents or general knowledge

### Example Queries
- "What is this document about?"
- "Summarize the main points"
- "What does it say about [specific topic]?"
- "Compare the information in document A vs document B"

## ğŸ“ Project Structure

```
RAGnarok/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py              # Flask application entry point
â”‚   â”œâ”€â”€ routes.py           # API endpoints
â”‚   â”œâ”€â”€ models.py           # Database models
â”‚   â”œâ”€â”€ rag_service.py      # RAG pipeline implementation
â”‚   â”œâ”€â”€ config.py           # Configuration settings
â”‚   â”œâ”€â”€ init_db.py          # Database initialization
â”‚   â”œâ”€â”€ migrate_db.py       # Database migration script
â”‚   â”œâ”€â”€ test_rag.py         # RAG testing utilities
â”‚   â”œâ”€â”€ requirements.txt    # Python dependencies
â”‚   â”œâ”€â”€ Dockerfile          # Backend container config
â”‚   â””â”€â”€ uploads/            # Uploaded PDF storage
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.js          # Main React component
â”‚   â”‚   â”œâ”€â”€ HomePage.jsx    # Main page layout
â”‚   â”‚   â”œâ”€â”€ UploadCard.jsx  # PDF upload interface
â”‚   â”‚   â”œâ”€â”€ PDFListCard.jsx # Document management
â”‚   â”‚   â”œâ”€â”€ LLMInteractCard.jsx # Chat interface
â”‚   â”‚   â”œâ”€â”€ FireBackground.jsx  # Animated background
â”‚   â”‚   â””â”€â”€ styles.css      # Custom styling
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ package.json        # Node.js dependencies
â”‚   â””â”€â”€ Dockerfile          # Frontend container config
â”œâ”€â”€ docker-compose.yml      # Multi-service orchestration
â””â”€â”€ README.md
```

## ğŸ”Œ API Endpoints

### Document Management
- `POST /api/upload` - Upload and process PDF files
- `GET /api/pdfs` - List all uploaded documents with status
- `GET /api/test` - Health check endpoint

### Chat & RAG
- `POST /api/llm` - Send questions and get RAG-enhanced responses
  ```json
  {
    "prompt": "Your question here",
    "use_rag": true  // optional, defaults to true
  }
  ```

### Admin Tools
- `GET /api/admin/flush` - Clear all documents and embeddings
- `POST /api/admin/reprocess` - Reprocess failed/unprocessed PDFs

## ğŸ›  Troubleshooting

### Common Issues

**PDF Processing Fails**
- Check if PDF contains extractable text (not just images)
- Try the reprocess endpoint: `POST /api/admin/reprocess`
- Check backend logs: `docker-compose logs backend`

**Ollama Model Download Slow**
- First run downloads ~4GB Llama3 model
- Check progress: `docker-compose logs ollama`
- Ensure stable internet connection

**Out of Memory Errors**
- Ensure at least 4GB RAM available
- Reduce chunk size in `rag_service.py` if needed
- Monitor usage: `docker stats`

**Vector Search Returns No Results**
- Check document processing status in sidebar
- Try broader/different question phrasing
- Verify embeddings: run `python test_rag.py` in backend container

### Development Commands

```bash
# View logs
docker-compose logs -f [service_name]

# Access backend container
docker exec -it ragnarok-backend-1 /bin/bash

# Run RAG pipeline tests
docker exec -it ragnarok-backend-1 python test_rag.py

# Reset everything
docker-compose down -v && docker-compose up --build
```

## âœ… Implementation Status

- âœ… **PDF Ingestion**: Automatic text extraction and processing
- âœ… **RAG Pipeline**: Vector embeddings with ChromaDB semantic search  
- âœ… **LLM Integration**: Ollama/Llama3 with context-aware responses
- âœ… **Web UI**: Modern React interface with real-time chat
- âœ… **Source Attribution**: Document provenance in all responses
- âœ… **Persistent Storage**: Vector DB and metadata persistence
- âœ… **Docker Deployment**: One-command multi-service setup
- âœ… **Admin Tools**: Document management and system maintenance

### Future Enhancements
- ğŸ”„ **Image Processing**: OCR for image-based PDFs
- ğŸ”„ **Multi-format Support**: Word docs, text files, web pages
- ğŸ”„ **User Authentication**: Multi-user document isolation
- ğŸ”„ **Advanced Search**: Filtering, metadata search, faceted queries
- ğŸ”„ **Cloud Deployment**: Kubernetes configs and cloud-native features
- ğŸ”„ **Performance Optimization**: Caching, async processing, load balancing

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“„ License

MIT License - see LICENSE file for details

---

**RAGnarok** - *When documents meet AI, knowledge becomes power* ğŸ”¥
